{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbfdd2-a0a5-4af3-a019-44d8db546acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\nlp-homework\\THUCNews\n",
      "['科技' '体育' '时政' '股票' '娱乐' '教育' '家居' '财经' '房产' '社会' '游戏' '彩票' '星座' '时尚']\n",
      "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\n",
      "不同类别的标签数量和占比如下：\n",
      "    ID   Count     Ratio\n",
      "科技   0  162245  0.194874\n",
      "体育   1  130982  0.157401\n",
      "时政   2   62867  0.075455\n",
      "股票   3  153949  0.184670\n",
      "娱乐   4   92228  0.110792\n",
      "教育   5   41680  0.050159\n",
      "家居   6   32363  0.038976\n",
      "财经   7   36963  0.044372\n",
      "房产   8   19922  0.023981\n",
      "社会   9   50541  0.060820\n",
      "游戏  10   24283  0.029152\n",
      "彩票  11    7598  0.009077\n",
      "星座  12    3515  0.004281\n",
      "时尚  13   13335  0.015990\n",
      "    ID   Count     Ratio      Weight  Weight Norm\n",
      "科技   0  162245  0.194874    5.131522     0.008467\n",
      "体育   1  130982  0.157401    6.353183     0.010482\n",
      "时政   2   62867  0.075455   13.252862     0.021867\n",
      "股票   3  153949  0.184670    5.415058     0.008935\n",
      "娱乐   4   92228  0.110792    9.025897     0.014892\n",
      "教育   5   41680  0.050159   19.936703     0.032895\n",
      "家居   6   32363  0.038976   25.657085     0.042333\n",
      "财经   7   36963  0.044372   22.536494     0.037184\n",
      "房产   8   19922  0.023981   41.699695     0.068802\n",
      "社会   9   50541  0.060820   16.442063     0.027129\n",
      "游戏  10   24283  0.029152   34.303018     0.056598\n",
      "彩票  11    7598  0.009077  110.171449     0.181778\n",
      "星座  12    3515  0.004281  233.614095     0.385452\n",
      "时尚  13   13335  0.015990   62.539146     0.103187\n",
      "The length information of Train&&Dev is as follows:/n\n",
      "count    832471.000000\n",
      "mean         19.388112\n",
      "std           4.097139\n",
      "min           2.000000\n",
      "25%          17.000000\n",
      "50%          20.000000\n",
      "75%          23.000000\n",
      "max          48.000000\n",
      "Name: text_a, dtype: float64\n",
      "The length information of Test is as follows:/n\n",
      "count    83599.000000\n",
      "mean        19.815022\n",
      "std          3.883845\n",
      "min          3.000000\n",
      "25%         17.000000\n",
      "50%         20.000000\n",
      "75%         23.000000\n",
      "max         84.000000\n",
      "Name: text_a, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-15 20:23:04,987] [    INFO] - Already cached C:\\Users\\22826\\.paddlenlp\\models\\roberta-wwm-ext-large\\roberta_chn_large.pdparams\n",
      "[2023-12-15 20:23:51,659] [    INFO] - Already cached C:\\Users\\22826\\.paddlenlp\\models\\roberta-wwm-ext-large\\vocab.txt\n",
      "[2023-12-15 20:23:51,674] [    INFO] - Already cached C:\\Users\\22826\\.paddlenlp\\models\\bert-wwm-ext-chinese\\bert-wwm-ext-chinese.pdparams\n",
      "[2023-12-15 20:24:06,186] [    INFO] - Already cached C:\\Users\\22826\\.paddlenlp\\models\\bert-wwm-ext-chinese\\bert-wwm-ext-chinese-vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.NewsData'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11760\n",
      "global step 10, epoch: 1, batch: 10, loss: 2.80805, avgLoss: 2.76367, acc: 0.07539\n",
      "global step 20, epoch: 1, batch: 20, loss: 2.86459, avgLoss: 2.76891, acc: 0.07656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 进入比赛数据集存放目录\n",
    "%cd THUCNews\n",
    "\n",
    "# 使用pandas读取数据集\n",
    "import pandas as pd\n",
    "train = pd.read_table('train.txt', sep='\\t',header=None)  # 训练集\n",
    "dev = pd.read_table('dev.txt', sep='\\t',header=None)      # 验证集（官方已经划分的）\n",
    "test = pd.read_table('test.txt', sep='\\t',header=None)    # 测试集\n",
    "# 对数据集手动添加列名\n",
    "train.columns = [\"text_a\",'label']\n",
    "dev.columns = [\"text_a\",'label']\n",
    "test.columns = [\"text_a\"]\n",
    "# 拼接训练和验证集，进行统计分析\n",
    "total = pd.concat([train,dev],axis=0)\n",
    "print(total.label.unique())\n",
    "print(total.label.unique().tolist())\n",
    "['科技' '体育' '时政' '股票' '娱乐' '教育' '家居' '财经' '房产' '社会' '游戏' '彩票' '星座' '时尚']\n",
    "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\n",
    "# 拼接训练和验证集，进行统计分析\n",
    "total = pd.concat([train,dev],axis=0)\n",
    "print(\"不同类别的标签数量和占比如下：\")\n",
    "# 得到不同类别的标签数量和占比，按照降序排列，数据类型是“pandas.core.series.Series”\n",
    "series_Count_desc = total['label'].value_counts()\n",
    "series_Ratio_desc = (train['label'].value_counts())/sum(train['label'].value_counts())\n",
    "# 合并不同类别的标签数量和占比，仍然是降序排列\n",
    "series_Info_desc = pd.concat([series_Count_desc,series_Ratio_desc],axis=1)\n",
    "# print(series_Info_desc)\n",
    "# print(series_Info_desc.keys())\n",
    "# 定义要进行分类的类别\n",
    "label_list = total.label.unique().tolist()\n",
    "# 建立由分类标签到ID的映射表\n",
    "Label_ID_map = {Val:ID for ID,Val in enumerate(label_list)}\n",
    "series_Info_data = pd.Series(Label_ID_map)\n",
    "# print(series_Info_data.keys())\n",
    "# 按照ID的顺序，重新排列不同类别的标签数量和占比\n",
    "df_Info_data = pd.concat([series_Info_data,series_Info_desc],axis=1)\n",
    "# 设置列名称\n",
    "df_Info_data.columns=['ID','Count','Ratio']\n",
    "print(df_Info_data)\n",
    "# print(type(df_Info_data))\n",
    "#计算损失函数不同类型的权重：占有比例的倒数，需要进行归一化\n",
    "series_Recip_desc = 1/series_Ratio_desc#倒数\n",
    "series_Weight_desc = series_Recip_desc\n",
    "series_WeightNorm_desc = series_Recip_desc/sum(series_Recip_desc)#归一化\n",
    "# print(series_Recip_desc)\n",
    "# print(series_Weight_desc)\n",
    "#合并到不同类别的标签数量和占比种\n",
    "df_Info_data = pd.concat([df_Info_data,series_Weight_desc],axis=1)\n",
    "df_Info_data = pd.concat([df_Info_data,series_WeightNorm_desc],axis=1)\n",
    "# 设置列名称\n",
    "df_Info_data.columns=['ID','Count','Ratio','Weight','Weight Norm']\n",
    "print(df_Info_data)\n",
    "# 全局设置解决matplotlib中文显示错误的问题，参考：https://aistudio.baidu.com/aistudio/projectdetail/1658980\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "# 设置显示中文\n",
    "matplotlib.rcParams['font.sans-serif'] = ['FZSongYi-Z13S'] # 指定默认字体\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题\n",
    "# 设置字体大小\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "\n",
    "# 绘制不同类型标签的分布情况(按照ID升序排列)\n",
    "plt.subplot2grid((2,2),(0,0),colspan=2)\n",
    "df_Info_data['Count'].plot(kind='bar');\n",
    "# 绘制不同类型标签的分布情况(按照样本数量降序排列)\n",
    "plt.subplot2grid((2,2),(1,0),colspan=2)\n",
    "series_Count_desc.plot(kind='bar');\n",
    "# 统计“训练集+验证集”的长度信息\n",
    "print(\"The length information of Train&&Dev is as follows:/n\")\n",
    "print(total['text_a'].map(len).describe())\n",
    "# 统计“测试集”的长度信息\n",
    "print(\"The length information of Test is as follows:/n\")\n",
    "print(test['text_a'].map(len).describe())\n",
    "# 是否使用全部数据集（训练集+验证集）进行训练\n",
    "useTotalData = False\n",
    "# 是否使用伪标签进行训练\n",
    "useFakeData = False\n",
    "# 是否仅使用伪标签进行训练（增量学习）\n",
    "useFakeOnly = False\n",
    "if useTotalData == True:\n",
    "    #使用全部数据集（训练集+验证集）进行训练\n",
    "    train = pd.concat([train,dev],axis=0)\n",
    "if useFakeData == True:\n",
    "    #使用伪标签进行训练\n",
    "    train = pd.concat([train,fakeData1],axis=0)\n",
    "if useFakeOnly == True:\n",
    "    #仅使用伪标签进行训练（增量学习）\n",
    "    train = fakeData1\n",
    "# 保存处理后的数据集文件\n",
    "train.to_csv('train.csv', sep='\\t', index=False)  # 保存训练集，格式为text_a,label，以\\t分隔开\n",
    "dev.to_csv('dev.csv', sep='\\t', index=False)      # 保存验证集，格式为text_a,label，以\\t分隔开\n",
    "test.to_csv('test.csv', sep='\\t', index=False)    # 保存测试集，格式为text_a，以\\t分隔开\n",
    "# 导入所需的第三方库\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url\n",
    "# 导入paddlenlp所需的相关包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder\n",
    "# 使用roberta-wwm-ext-large模型\n",
    "MODEL_NAME = \"roberta-wwm-ext-large\"\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14) # 此次分类任务为14分类任务，故num_classes设置为14\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# 使用bert-wwm-ext-chinese模型\n",
    "MODEL_NAME = \"bert-wwm-ext-chinese\"\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14) # 此次分类任务为14分类任务，故num_classes设置为14\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    #训练集和验证集对应文件名称\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',      # 验证集\n",
    "    }\n",
    "\n",
    "    #获取训练集和验证集的文件名称\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    #从文件中读取数据\n",
    "    def _read(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\n",
    "\n",
    "    #获取类别标签\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签\n",
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    print(reader_cls)\n",
    "    #数据集加载实例\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "    \n",
    "    #通过实例加载数据集\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n",
    "# 加载训练和验证集\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])\n",
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    #如果不是测试集，则需要返回标签\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:#测试集不需要返回标签\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "    # 参数设置：\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值\n",
    "# 注意该场景下若使用nezha-large-wwm-chinese需将batch_size修改为256，chinese-xlnet-large修改为128，其他模型则为300。否则容易出现爆显存问题\n",
    "batch_size = 256\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。 通过文本长度分析可以看出文本长度最大为48，故此处设置为48\n",
    "max_seq_length = 48\n",
    "    # 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 验证集迭代器\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "#按照Focal Loss损失函数定义计算，发现效果不如Paddle的交叉熵\n",
    "import paddle.nn.functional as F\n",
    "class MultiCEFocalLoss(nn.Layer):\n",
    "    #class_num：类型数量，reduction：如何处理不同类型的损失函数贡献，可以选择：平均值(mean)、总和(sum)，默认mean。\n",
    "    #use_softmax：是否对输入数据进行softmax处理\n",
    "    def __init__(self, class_num, gamma=2, alpha=None, reduction='mean',use_softmax=True):\n",
    "        super(MultiCEFocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = paddle.ones((class_num, 1))\n",
    "        elif isinstance(alpha,list):\n",
    "            self.alpha = paddle.to_tensor(alpha,dtype=paddle.float32)\n",
    "        elif isinstance(alpha,[float,int]):\n",
    "            self.alpha = paddle.to_tensor([alpha])\n",
    "  \n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.class_num =  class_num\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        #print(\"predict\",preds)\n",
    "        #print(\"label\",labels)\n",
    "        labels = labels.cast(paddle.int64)\n",
    "        # print(\"The dimension of preds is:\",preds.ndim)\n",
    "        # print(\"The dimension of labels is:\",labels.ndim)\n",
    "        if labels.ndim > 1:#去除labels中单值数组的维度\n",
    "            labels = labels.squeeze(1)\n",
    "        #样本中每个类型预测值构成的List\n",
    "        if self.use_softmax:#基于softmmax的预测概率\n",
    "            pt = F.softmax(preds,axis=-1)\n",
    "        else:\n",
    "            pt = preds\n",
    "        # print(\"Processed predict\",pt)\n",
    "        #样本标签的One Hot编码\n",
    "        class_mask = F.one_hot(labels, self.class_num)\n",
    "        # print(\"One hot of each Label ID\",class_mask)\n",
    "        #转换为1列，元素为样本标签的ID\n",
    "        ids = labels.reshape((-1, 1))\n",
    "        # print(\"Label's ID\",ids)\n",
    "        # print(\"Label's alpha\",self.alpha)\n",
    "        #样本标签类型对应权重构成的List，长度等于样本个数\n",
    "        alpha = self.alpha[ids.reshape((-1,1))].reshape((-1,1))\n",
    "        # print(\"Label's alpha of each sample\",alpha)\n",
    "        #样本标签类型对应预测值(来自pt)构成的List，长度等于样本个数\n",
    "        probs = (pt * class_mask).sum(1).reshape((-1, 1)) # 利用onehot作为mask，提取对应的pt\n",
    "        # print(\"Label's predict of each sample\",probs)\n",
    "        log_p = probs.log()\n",
    "        # print(\"Label's log predict of each sample\",log_p)\n",
    "        #融合样本标签类型的权重alpha、预测值对数log_p、动态衰减因子pow[(1-预测值), gama]，计算损失函数\n",
    "        loss = -alpha * (paddle.pow((1 - probs), self.gamma)) * log_p\n",
    "        # print(loss)\n",
    "        \n",
    "        #融合不同类型的损失函数贡献\n",
    "        if self.reduction == 'mean':#平均值\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':#总和\n",
    "            loss = loss.sum()\n",
    "        return loss\n",
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup, CosineDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 4\n",
    "# 学习率预热比例，用于控制“学习率-Step次数”曲线中峰值点的Step位置，此处是在整个Step的10%位置到达峰值，然后衰减。\n",
    "warmup_proportion = 0.1\n",
    "# 逐层权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.0\n",
    "\n",
    "#训练过程中总共经历的Step数量\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "print(num_training_steps)\n",
    "#线性预热学习率，在开始的“warmup_proportion*num_training_steps”个Step中，学习率由0线性增加到learning_rate，然后再余弦衰减到0。\n",
    "lr_scheduler = CosineDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "#自定义Focal Loss函数\n",
    "#criterion = MultiCEFocalLoss(class_num=14, alpha=(df_Info_data['Weight'].values.tolist()))\n",
    "#交叉熵损失函数\n",
    "criterion = paddle.nn.CrossEntropyLoss(weight=paddle.to_tensor(df_Info_data['Weight'].values.astype(\"float32\")))\n",
    "#criterion = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率\n",
    "# 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)\n",
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#验证集上准确率最高模型的保存路径\n",
    "save_dir = \"checkpoint\"\n",
    "if not  os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "#最后1个Epoch结束后模型的保存路径\n",
    "save_final_dir = \"checkpoint_final\"\n",
    "if not  os.path.exists(save_final_dir):\n",
    "    os.makedirs(save_final_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "#记录效果更优时的Step和Epoch数值\n",
    "best_global_step = 0\n",
    "best_epoch = 0\n",
    "#逐个Epoch进行处理\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = []\n",
    "    #逐个Step进行处理\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        # print(len(logits[0]))\n",
    "        # print(len(labels[0]))\n",
    "        # print(logits)\n",
    "        # print(labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        #每隔10个Step，统计结果并输出\n",
    "        if global_step % 10 == 0:\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, avgLoss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, np.mean(losses), acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if useTotalData == False:\n",
    "            #没有使用全部数据集（训练集+验证集）进行训练，则每隔一定的批次数后进行评估，避免遗漏中间的最佳模型\n",
    "            if global_step % 300 == 0:\n",
    "                accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "                if accu > pre_accu:\n",
    "                    # 保存效果更优的模型参数\n",
    "                    save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "                    pre_accu=accu\n",
    "                    #记录效果更优时的Step和Epoch数值\n",
    "                    best_global_step = global_step\n",
    "                    best_epoch = epoch\n",
    "                    print(\"The best model is found in epoch: %d, batch: %d\" % (best_epoch, best_global_step))\n",
    "    \n",
    "    if useTotalData == False:\n",
    "        #没有使用全部数据集（训练集+验证集）进行训练，则每轮结束对验证集进行评估\n",
    "        accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "        print(accu)\n",
    "        if accu > pre_accu:\n",
    "            #保存较上一轮效果更优的模型参数\n",
    "            save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "            paddle.save(model.state_dict(), save_param_path)\n",
    "            pre_accu=accu\n",
    "            #记录效果更优时的Step和Epoch数值\n",
    "            best_global_step = global_step\n",
    "            best_epoch = epoch\n",
    "        print(\"The best model is found in epoch: %d, batch: %d\" % (best_epoch, best_global_step))\n",
    "    else:#保存本轮的模型参数，避免中间宕机\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    #绘制损失函数曲线\n",
    "    plt.figure()\n",
    "    #曲线标题\n",
    "    plt.title(\"The loss at the different step\")\n",
    "    #曲线X轴和Y轴名称\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(losses)\n",
    "    plt.figure()\n",
    "\n",
    "#最后1个Epoch结束后，保存模型\n",
    "save_final_param_path = os.path.join(save_final_dir, 'model_state.pdparams')\n",
    "paddle.save(model.state_dict(), save_final_param_path)\n",
    "tokenizer.save_pretrained(save_final_dir)\n",
    "# 定义要进行分类的类别\n",
    "label_list = train.label.unique().tolist()\n",
    "label_map = { \n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "print(label_map)\n",
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # 将数据集分割为多个批次(Batch)，保存在batches中\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    #逐个处理样本\n",
    "    for example in examples:\n",
    "        #将每个样本添加到one_batch中，如果个数增加到batch size，那么做为1个批次添加到batches中\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        #处理最后1个one_batch的元素个数小于batch size的特殊情况\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    #逐批次处理数据\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results  # 返回预测结果\n",
    "\n",
    "# 定义对数据的预处理函数,处理为模型输入指定list格式\n",
    "def preprocess_prediction_data(data):\n",
    "    examples = []\n",
    "    for text_a in data:\n",
    "        examples.append({\"text_a\": text_a})\n",
    "    return examples\n",
    "\n",
    "# 将list格式的预测结果存储为txt文件，提交格式要求：每行一个类别\n",
    "def write_results(labels, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.writelines(\"\\n\".join(labels))\n",
    "# 读取要进行预测的测试集文件\n",
    "test = pd.read_csv('./test.csv',sep='\\t')\n",
    "\n",
    "# 对测试集数据进行格式处理\n",
    "test_data = list(test.text_a)\n",
    "test_example = preprocess_prediction_data(test_data)\n",
    "# 对测试集进行预测\n",
    "results = predict(model, test_example, tokenizer, label_map, batch_size=16) \n",
    "#保存预测结果为符合格式要求的txt文件\n",
    "write_results(results, \"./result.txt\")\n",
    "#压缩为zip文件\n",
    "!zip 'submission.zip' 'result.txt'\n",
    "!cp -r submission.zip \n",
    "# 加载在验证集上效果最优的一轮的模型参数\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint_final/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "# 测试最优模型参数在验证集上的分数\n",
    "evaluate(model, criterion, metric, dev_data_loader)\n",
    "# 加载在验证集上效果最优的一轮的模型参数\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint_final/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "# 测试最优模型参数在验证集上的分数\n",
    "evaluate(model, criterion, metric, dev_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
